{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fd7163",
   "metadata": {},
   "source": [
    "IND 320 - NMBU\n",
    "\n",
    "Project work, part 2 - Data Sources\n",
    "\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661c8ee",
   "metadata": {},
   "source": [
    "## AI usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d1e65",
   "metadata": {},
   "source": [
    "## Log describing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf289ab",
   "metadata": {},
   "source": [
    "## Github and Streamlit app links\n",
    "\n",
    "- Streamlit app: [https://liserochat-ind320-dashboard.streamlit.app\n",
    "](https://liserochat-ind320-dashboard.streamlit.app)  \n",
    "- GitHub repository: [https://github.com/lise-dev/liserochat-ind320-dashboard.git](https://github.com/lise-dev/liserochat-ind320-dashboard.git)\n",
    "\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcc751d",
   "metadata": {},
   "source": [
    "## 1. Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b025ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b119d",
   "metadata": {},
   "source": [
    "## 2. Define constants\n",
    "\n",
    "I define the constants that will be used throughout the notebook.\n",
    "\n",
    "They include:\n",
    "- API parameters for fetching data from Elhub.\n",
    "- Lists of price areas, production groups, and months for 2021.\n",
    "- MongoDB connection details for storing the processed dataset online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3e0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API settings\n",
    "API_BASE = \"https://api.elhub.no/energy-data/v0/price-areas\"  \n",
    "DATASET  = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "\n",
    "# Production and area constants\n",
    "PRICE_AREAS = [\"NO1\",\"NO2\",\"NO3\",\"NO4\",\"NO5\"]\n",
    "PROD_GROUPS = [\"solar\",\"hydro\",\"wind\",\"thermal\",\"other\"]\n",
    "MONTHS = [f\"2021-{m:02d}\" for m in range(1,13)]\n",
    "\n",
    "# MongoDB connection details\n",
    "MONGO_URI = \"mongodb+srv://rochatlise17_db_user:7Ydw0jbNZLMWKhb@ind320.nrhhfgb.mongodb.net/?retryWrites=true&w=majority&appName=ind320\"\n",
    "MONGO_DB = \"ind320\"\n",
    "MONGO_COLLECTION = \"elhub_production_2021\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b360bb4",
   "metadata": {},
   "source": [
    "## 3. Define helper for monthly time ranges\n",
    "\n",
    "The Elhub API requires UTC timestamps for the start and end of each request.  \n",
    "I created a helper function that takes a year-month string (for example `\"2021-01\"`) and returns the corresponding start and end timestamps in UTC format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d711dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_range_utc(ym: str):\n",
    "    start = pd.Timestamp(f\"{ym}-01 00:00:00\", tz=\"UTC\")\n",
    "    end   = (start + pd.offsets.MonthEnd(1)) + pd.Timedelta(days=1)\n",
    "    \n",
    "    def fmt(ts):\n",
    "        s = ts.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        return s[:-2] + \":\" + s[-2:]\n",
    "    \n",
    "    return fmt(start), fmt(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5849b73",
   "metadata": {},
   "source": [
    "## 4. Fetch hourly production data for one area and month\n",
    "\n",
    "This function sends a request to the Elhub API for one production group, one price area, and one month.  \n",
    "It builds the correct query parameters (`priceArea`, `startDate`, `endDate`, `productionGroup`), retrieves the JSON response, and converts the relevant data into a clean Pandas DataFrame.\n",
    "\n",
    "The returned dataframe contains the following columns:\n",
    "- `price_area`\n",
    "- `production_group`\n",
    "- `start_time`\n",
    "- `quantity_kwh`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a374160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_month_one_group(area: str, ym: str, group: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch hourly production data for a specific area, month and production group.\"\"\"\n",
    "    \n",
    "    start = pd.Timestamp(f\"{ym}-01\").strftime(\"%Y-%m-%d\")\n",
    "    end = (pd.Timestamp(f\"{ym}-01\") + pd.offsets.MonthEnd(1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    url = f\"https://api.elhub.no/energy-data/v0/price-areas/{area}\"\n",
    "    params = {\n",
    "        \"dataset\": \"PRODUCTION_PER_GROUP_MBA_HOUR\",\n",
    "        \"startDate\": start,\n",
    "        \"endDate\": end,\n",
    "        \"productionGroup\": group,\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    data_json = response.json()\n",
    "\n",
    "    data = data_json.get(\"data\", [])\n",
    "    if not data:\n",
    "        return pd.DataFrame(columns=[\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"])\n",
    "    \n",
    "    attributes = data[0].get(\"attributes\", {})\n",
    "    items = attributes.get(\"productionPerGroupMbaHour\", [])\n",
    "\n",
    "    if not items:\n",
    "        return pd.DataFrame(columns=[\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"])\n",
    "\n",
    "    df = (\n",
    "        pd.json_normalize(items)[[\"priceArea\", \"productionGroup\", \"startTime\", \"quantityKwh\"]]\n",
    "        .rename(columns={\n",
    "            \"priceArea\": \"price_area\",\n",
    "            \"productionGroup\": \"production_group\",\n",
    "            \"startTime\": \"start_time\",\n",
    "            \"quantityKwh\": \"quantity_kwh\",\n",
    "        })\n",
    "    )\n",
    "\n",
    "    df[\"start_time\"] = pd.to_datetime(df[\"start_time\"], utc=True, errors=\"coerce\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1df158",
   "metadata": {},
   "source": [
    "## 5. Loop over all areas and months of 2021\n",
    "\n",
    "In this step, I loop through all five Norwegian price areas and the twelve months of 2021.  \n",
    "For each area, month, and production group, the `fetch_month_one_group()` function is called.  \n",
    "All the resulting DataFrames are combined into a single dataset called `raw_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27055d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "\n",
    "for area in PRICE_AREAS:\n",
    "    for ym in MONTHS:\n",
    "        for g in PROD_GROUPS:\n",
    "            try:\n",
    "                dfm = fetch_month_one_group(area, ym, g)\n",
    "                if not dfm.empty:\n",
    "                    all_chunks.append(dfm)\n",
    "                else:\n",
    "                    print(f\"Empty result for {area} {ym} {g}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed for {area} {ym} {g}: {e}\")\n",
    "\n",
    "if all_chunks:\n",
    "    raw_df = pd.concat(all_chunks, ignore_index=True)\n",
    "else:\n",
    "    raw_df = pd.DataFrame(columns=[\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"])\n",
    "\n",
    "print(\"Total number of rows and columns:\", raw_df.shape)\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b03e5a",
   "metadata": {},
   "source": [
    "## 6. Save the raw data to CSV\n",
    "\n",
    "Once all data are collected, I export the full dataset to a CSV file.  \n",
    "This file will be used later in Spark (for Cassandra) and in the Streamlit app.\n",
    "\n",
    "The CSV is saved in the project data folder:\n",
    "`/home/lse/Documents/IND320/liserochat-ind320-dashboard/app/data/elhub_production_2021_raw.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35a3507",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(\"/home/lse/Documents/IND320/liserochat-ind320-dashboard/app/data/elhub_production_2021_raw.csv\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if raw_df.empty:\n",
    "    print(\"No CSV file was created.\")\n",
    "else:\n",
    "    raw_df.to_csv(out_path, index=False)\n",
    "    print(f\"CSV file successfully saved to: {out_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b52e6b",
   "metadata": {},
   "source": [
    "## 7. Initialize Spark with the Cassandra connector\n",
    "\n",
    "Here I create a `SparkSession` configured to connect to the local Cassandra instance running in Docker  \n",
    "(on `127.0.0.1:9042`). The Spark Cassandra connector is automatically downloaded via  \n",
    "`spark.jars.packages` when the session starts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any previous Spark session if running\n",
    "try:\n",
    "    spark.stop()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Create a new Spark session configured for Cassandra\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IND320-Elhub-2021\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.13:3.5.0\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\")\n",
    "    .config(\"spark.cassandra.output.consistency.level\", \"LOCAL_ONE\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4600d797",
   "metadata": {},
   "source": [
    "## 8. Prepare the dataframe we want to persist in Cassandra\n",
    "\n",
    "Before inserting data into Cassandra, I check that the dataframe `raw_df` exists,  \n",
    "is not empty, and includes the expected columns with correct names:\n",
    "- `price_area`\n",
    "- `production_group`\n",
    "- `start_time`\n",
    "- `quantity_kwh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e067479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the dataframe is ready for Cassandra\n",
    "if \"raw_df\" not in globals():\n",
    "    raise ValueError(\"raw_df is not defined. Please run the previous steps first.\")\n",
    "\n",
    "if raw_df.empty:\n",
    "    raise ValueError(\"raw_df is empty. Fetch data from the API before continuing.\")\n",
    "\n",
    "required_cols = [\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\"]\n",
    "for col in required_cols:\n",
    "    if col not in raw_df.columns:\n",
    "        raise ValueError(f\"Missing column: {col}\")\n",
    "\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b9761",
   "metadata": {},
   "source": [
    "## 9. Write the data into Cassandra using Spark\n",
    "\n",
    "Here I convert the pandas dataframe (`raw_df`) to a Spark DataFrame  \n",
    "and insert it into the Cassandra keyspace **elhub**, table **production_2021**.\n",
    "\n",
    "I use `mode(\"append\")` to keep existing rows and only add new ones.  \n",
    "The Cassandra table schema was already created with the primary key  \n",
    "`(price_area, start_time, production_group)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab978bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Cassandra connection parameters for Spark\n",
    "spark.conf.set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "spark.conf.set(\"spark.cassandra.connection.port\", \"9042\")\n",
    "spark.conf.set(\"spark.cassandra.output.consistency.level\", \"LOCAL_ONE\")\n",
    "\n",
    "# Basic checks before writing\n",
    "print(\"Columns:\", raw_df.columns.tolist())\n",
    "print(\"Types:\\n\", raw_df.dtypes.head())\n",
    "print(\"Rows:\", len(raw_df))\n",
    "\n",
    "# Convert pandas dataframe to Spark dataframe\n",
    "sdf = spark.createDataFrame(raw_df)\n",
    "\n",
    "# Write to Cassandra\n",
    "try:\n",
    "    (\n",
    "        sdf.write\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\n",
    "        .mode(\"append\")\n",
    "        .options(keyspace=\"elhub\", table=\"production_2021\")\n",
    "        .save()\n",
    "    )\n",
    "    print(\"Data successfully written to elhub.production_2021\")\n",
    "except Exception as e:\n",
    "    print(\"Write failed:\", e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcfac5a",
   "metadata": {},
   "source": [
    "## 10. Read back from Cassandra and inspect\n",
    "\n",
    "I now read the table `elhub.production_2021` back from Cassandra using Spark.  \n",
    "This allows me to confirm that the data was correctly written.  \n",
    "Then, I convert it to a pandas dataframe for later use (for plotting and MongoDB upload).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cae57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Cassandra\n",
    "sdf_check = (\n",
    "    spark.read\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(keyspace=\"elhub\", table=\"production_2021\")\n",
    "    .load()\n",
    "    .select(\"price_area\", \"production_group\", \"start_time\", \"quantity_kwh\")\n",
    ")\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "pdf = sdf_check.toPandas()\n",
    "\n",
    "print(\"Data shape:\", pdf.shape)\n",
    "pdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e369d",
   "metadata": {},
   "source": [
    "## 11. Insert the curated data into MongoDB\n",
    "\n",
    "In this step I push the cleaned dataset to MongoDB Atlas.\n",
    "\n",
    "The goal is:\n",
    "- Store the data in a cloud database (`ind320.elhub_production_2021`)\n",
    "- Let the Streamlit app read directly from MongoDB (so the app does not have to connect to Cassandra)\n",
    "\n",
    "I use `pymongo` to connect to the cluster.  \n",
    "The connection string is stored in my local secrets and is not committed to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection details:\n",
    "# I keep the URI in a local variable here, but in production Streamlit reads it from st.secrets.\n",
    "client = MongoClient(MONGO_URI)\n",
    "mongo_db = client[MONGO_DB]\n",
    "mongo_collection = mongo_db[MONGO_COLLECTION]\n",
    "\n",
    "# Quick check that the connection works\n",
    "print(\"MongoDB collections:\", mongo_db.list_collection_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
