{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fd7163",
   "metadata": {},
   "source": [
    "IND 320 - NMBU\n",
    "\n",
    "Project work, part 2 - Data Sources\n",
    "\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e661c8ee",
   "metadata": {},
   "source": [
    "## AI usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d1e65",
   "metadata": {},
   "source": [
    "## Log describing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf289ab",
   "metadata": {},
   "source": [
    "## Github and Streamlit app links\n",
    "\n",
    "- Streamlit app: [https://liserochat-ind320-dashboard.streamlit.app\n",
    "](https://liserochat-ind320-dashboard.streamlit.app)  \n",
    "- GitHub repository: [https://github.com/lise-dev/liserochat-ind320-dashboard.git](https://github.com/lise-dev/liserochat-ind320-dashboard.git)\n",
    "\n",
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcc751d",
   "metadata": {},
   "source": [
    "## 1. Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b025ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b119d",
   "metadata": {},
   "source": [
    "## 2. Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c3e0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_BASE = \"https://api.elhub.no/energy-data/v0/price-areas\"  \n",
    "DATASET  = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "PRICE_AREAS = [\"NO1\",\"NO2\",\"NO3\",\"NO4\",\"NO5\"]\n",
    "PROD_GROUPS = [\"solar\",\"hydro\",\"wind\",\"thermal\",\"other\"]\n",
    "MONTHS = [f\"2021-{m:02d}\" for m in range(1,13)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b360bb4",
   "metadata": {},
   "source": [
    "## 3. Define helper for monthly time ranges\n",
    "\n",
    "The Elhub API uses UTC timestamps.  \n",
    "This function takes a year-month (e.g., `2021-01`) and returns a start and end date string for that month in UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d711dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_range_utc(ym: str):\n",
    "    start = pd.Timestamp(f\"{ym}-01 00:00:00\", tz=\"UTC\")\n",
    "    end   = (start + pd.offsets.MonthEnd(1)) + pd.Timedelta(days=1)\n",
    "    def fmt(ts):\n",
    "        s = ts.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
    "        return s[:-2] + \":\" + s[-2:]\n",
    "    return fmt(start), fmt(end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5849b73",
   "metadata": {},
   "source": [
    "## 4. Fetch hourly production data for one area and month\n",
    "\n",
    "This function:\n",
    "1. Builds the API query with `priceArea`, `startTime`, and `endTime`.\n",
    "2. Sends a request to the Elhub endpoint.\n",
    "3. Extracts the `productionPerGroupMbaHour` list.\n",
    "4. Converts it into a clean Pandas DataFrame with columns:\n",
    "   - `price_area`\n",
    "   - `production_group`\n",
    "   - `start_time`\n",
    "   - `quantity_kwh`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a374160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_month_one_group(area: str, ym: str, group: str) -> pd.DataFrame:\n",
    "    # yyyy-mm â†’ date-only (API accepte ce format)\n",
    "    start = pd.Timestamp(f\"{ym}-01\").strftime(\"%Y-%m-%d\")\n",
    "    end   = (pd.Timestamp(f\"{ym}-01\") + pd.offsets.MonthEnd(1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    url = f\"https://api.elhub.no/energy-data/v0/price-areas/{area}\"\n",
    "    params = {\n",
    "        \"dataset\": \"PRODUCTION_PER_GROUP_MBA_HOUR\",\n",
    "        \"startDate\": start,\n",
    "        \"endDate\": end,\n",
    "        \"productionGroup\": group,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "\n",
    "    data = js.get(\"data\", [])\n",
    "    if not data:\n",
    "        return pd.DataFrame(columns=[\"price_area\",\"production_group\",\"start_time\",\"quantity_kwh\"])\n",
    "    attrs = data[0].get(\"attributes\", {})\n",
    "    items = attrs.get(\"productionPerGroupMbaHour\", [])\n",
    "\n",
    "    if not items:\n",
    "        return pd.DataFrame(columns=[\"price_area\",\"production_group\",\"start_time\",\"quantity_kwh\"])\n",
    "\n",
    "    df = (pd.json_normalize(items)[[\"priceArea\",\"productionGroup\",\"startTime\",\"quantityKwh\"]]\n",
    "            .rename(columns={\n",
    "                \"priceArea\":\"price_area\",\n",
    "                \"productionGroup\":\"production_group\",\n",
    "                \"startTime\":\"start_time\",\n",
    "                \"quantityKwh\":\"quantity_kwh\",\n",
    "            }))\n",
    "\n",
    "    df[\"start_time\"] = pd.to_datetime(df[\"start_time\"], utc=True, errors=\"coerce\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1df158",
   "metadata": {},
   "source": [
    "## 5. Loop over all areas and months of 2021\n",
    "\n",
    "We now loop through all five Norwegian price areas and all twelve months of 2021.  \n",
    "For each combination, we call `fetch_month()` and concatenate the resulting data frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d27055d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMPTY NO5 2021-01 wind\n",
      "EMPTY NO5 2021-02 wind\n",
      "EMPTY NO5 2021-03 wind\n",
      "EMPTY NO5 2021-04 wind\n",
      "EMPTY NO5 2021-05 wind\n",
      "TOTAL SHAPE: (208248, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_area</th>\n",
       "      <th>production_group</th>\n",
       "      <th>start_time</th>\n",
       "      <th>quantity_kwh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2020-12-31 23:00:00+00:00</td>\n",
       "      <td>6.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2021-01-01 00:00:00+00:00</td>\n",
       "      <td>4.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2021-01-01 01:00:00+00:00</td>\n",
       "      <td>3.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2021-01-01 02:00:00+00:00</td>\n",
       "      <td>8.146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO1</td>\n",
       "      <td>solar</td>\n",
       "      <td>2021-01-01 03:00:00+00:00</td>\n",
       "      <td>8.616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  price_area production_group                start_time  quantity_kwh\n",
       "0        NO1            solar 2020-12-31 23:00:00+00:00         6.106\n",
       "1        NO1            solar 2021-01-01 00:00:00+00:00         4.030\n",
       "2        NO1            solar 2021-01-01 01:00:00+00:00         3.982\n",
       "3        NO1            solar 2021-01-01 02:00:00+00:00         8.146\n",
       "4        NO1            solar 2021-01-01 03:00:00+00:00         8.616"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks = []\n",
    "for area in PRICE_AREAS:\n",
    "    for ym in MONTHS:\n",
    "        for g in PROD_GROUPS:\n",
    "            try:\n",
    "                dfm = fetch_month_one_group(area, ym, g)\n",
    "                if not dfm.empty:\n",
    "                    all_chunks.append(dfm)\n",
    "                    # print(f\"OK  {area} {ym} {g}: {len(dfm)}\")\n",
    "                else:\n",
    "                    print(f\"EMPTY {area} {ym} {g}\")\n",
    "            except Exception as e:\n",
    "                print(f\"FAIL {area} {ym} {g}: {e}\")\n",
    "\n",
    "raw_df = (pd.concat(all_chunks, ignore_index=True)\n",
    "          if all_chunks else pd.DataFrame(columns=[\"price_area\",\"production_group\",\"start_time\",\"quantity_kwh\"]))\n",
    "print(\"TOTAL SHAPE:\", raw_df.shape)\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b03e5a",
   "metadata": {},
   "source": [
    "## 6. Save the raw data to CSV\n",
    "\n",
    "For later use (in Spark and Streamlit),  \n",
    "we export the full dataset to `data/elhub_production_2021_raw.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a35a3507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CSV to /home/lse/Documents/IND320/liserochat-ind320-dashboard/data/elhub_production_2021_raw.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "out_path = Path(\"../data/elhub_production_2021_raw.csv\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "if raw_df.empty:\n",
    "    print(\"DataFrame is empty; CSV not written.\")\n",
    "else:\n",
    "    raw_df.to_csv(out_path, index=False)\n",
    "    print(f\"Saved CSV to {out_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b52e6b",
   "metadata": {},
   "source": [
    "## 7. Initialize Spark with the Cassandra connector\n",
    "\n",
    "We create a SparkSession configured to talk to our local Cassandra (Docker) on `127.0.0.1:9042`.  \n",
    "The connector package is pulled automatically via `spark.jars.packages`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f40db5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/24 12:54:35 WARN Utils: Your hostname, lse-Creator-Z17-A12UGST, resolves to a loopback address: 127.0.1.1; using 10.20.3.60 instead (on interface wlo1)\n",
      "25/10/24 12:54:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/lse/Documents/IND320/liserochat-ind320-dashboard/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/lse/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/lse/.ivy2.5.2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e8bcaa0a-e05b-4485-83d5-65ca33c11e73;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.4.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.13.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.0 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.13.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.13.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.11 in central\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.12/3.4.1/spark-cassandra-connector_2.12-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.12;3.4.1!spark-cassandra-connector_2.12.jar (286ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector-driver_2.12/3.4.1/spark-cassandra-connector-driver_2.12-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1!spark-cassandra-connector-driver_2.12.jar (195ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-collection-compat_2.12/2.11.0/scala-collection-compat_2.12-2.11.0.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang.modules#scala-collection-compat_2.12;2.11.0!scala-collection-compat_2.12.jar (115ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-core-shaded/4.13.0/java-driver-core-shaded-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-core-shaded;4.13.0!java-driver-core-shaded.jar (846ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-mapper-runtime/4.13.0/java-driver-mapper-runtime-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-mapper-runtime;4.13.0!java-driver-mapper-runtime.jar(bundle) (65ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.10/commons-lang3-3.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.10!commons-lang3.jar (114ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.8!paranamer.jar(bundle) (95ms)\n",
      "downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.11/scala-reflect-2.12.11.jar ...\n",
      "\t[SUCCESSFUL ] org.scala-lang#scala-reflect;2.12.11!scala-reflect.jar (428ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/native-protocol/1.5.0/native-protocol-1.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#native-protocol;1.5.0!native-protocol.jar(bundle) (117ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-shaded-guava/25.1-jre-graal-sub-1/java-driver-shaded-guava-25.1-jre-graal-sub-1.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1!java-driver-shaded-guava.jar (346ms)\n",
      "downloading https://repo1.maven.org/maven2/com/typesafe/config/1.4.1/config-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] com.typesafe#config;1.4.1!config.jar(bundle) (109ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.26/slf4j-api-1.7.26.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.26!slf4j-api.jar (100ms)\n",
      "downloading https://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-core/4.1.18/metrics-core-4.1.18.jar ...\n",
      "\t[SUCCESSFUL ] io.dropwizard.metrics#metrics-core;4.1.18!metrics-core.jar(bundle) (152ms)\n",
      "downloading https://repo1.maven.org/maven2/org/hdrhistogram/HdrHistogram/2.1.12/HdrHistogram-2.1.12.jar ...\n",
      "\t[SUCCESSFUL ] org.hdrhistogram#HdrHistogram;2.1.12!HdrHistogram.jar(bundle) (125ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.3/reactive-streams-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.3!reactive-streams.jar (81ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (63ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/spotbugs/spotbugs-annotations/3.1.12/spotbugs-annotations-3.1.12.jar ...\n",
      "\t[SUCCESSFUL ] com.github.spotbugs#spotbugs-annotations;3.1.12!spotbugs-annotations.jar (81ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (107ms)\n",
      "downloading https://repo1.maven.org/maven2/com/datastax/oss/java-driver-query-builder/4.13.0/java-driver-query-builder-4.13.0.jar ...\n",
      "\t[SUCCESSFUL ] com.datastax.oss#java-driver-query-builder;4.13.0!java-driver-query-builder.jar(bundle) (95ms)\n",
      ":: resolution report :: resolve 6944ms :: artifacts dl 3536ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.13.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.0 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.4.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.4.1 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.11 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   19  |   19  |   19  |   0   ||   19  |   19  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e8bcaa0a-e05b-4485-83d5-65ca33c11e73\n",
      "\tconfs: [default]\n",
      "\t19 artifacts copied, 0 already retrieved (18359kB/13ms)\n",
      "25/10/24 12:54:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.20.3.60:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>IND320-Elhub-2021</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a7ba7910980>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IND320-Elhub-2021\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.4.1\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
