














import os
from pathlib import Path
import pandas as pd
import requests
from datetime import datetime
from pyspark.sql import SparkSession
from pymongo import MongoClient
import pandas as pd
import matplotlib.pyplot as plt





# API settings
API_BASE = "https://api.elhub.no/energy-data/v0/price-areas"  
DATASET  = "PRODUCTION_PER_GROUP_MBA_HOUR"

# Production and area constants
PRICE_AREAS = ["NO1","NO2","NO3","NO4","NO5"]
PROD_GROUPS = ["solar","hydro","wind","thermal","other"]
MONTHS = [f"2021-{m:02d}" for m in range(1,13)]

# MongoDB connection details
MONGO_URI = "mongodb+srv://@:@@cluster0.ilespbc.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
MONGO_DB = "ind320"
MONGO_COLLECTION = "elhub_production_2021"





def month_range_utc(ym: str):
    start = pd.Timestamp(f"{ym}-01 00:00:00", tz="UTC")
    end   = (start + pd.offsets.MonthEnd(1)) + pd.Timedelta(days=1)
    
    def fmt(ts):
        s = ts.strftime("%Y-%m-%dT%H:%M:%S%z")
        return s[:-2] + ":" + s[-2:]
    
    return fmt(start), fmt(end)





def fetch_month_one_group(area: str, ym: str, group: str) -> pd.DataFrame:
    """Fetch hourly production data for a specific area, month and production group."""
    
    start = pd.Timestamp(f"{ym}-01").strftime("%Y-%m-%d")
    end = (pd.Timestamp(f"{ym}-01") + pd.offsets.MonthEnd(1)).strftime("%Y-%m-%d")

    url = f"https://api.elhub.no/energy-data/v0/price-areas/{area}"
    params = {
        "dataset": "PRODUCTION_PER_GROUP_MBA_HOUR",
        "startDate": start,
        "endDate": end,
        "productionGroup": group,
    }

    response = requests.get(url, params=params, timeout=60)
    response.raise_for_status()
    data_json = response.json()

    data = data_json.get("data", [])
    if not data:
        return pd.DataFrame(columns=["price_area", "production_group", "start_time", "quantity_kwh"])
    
    attributes = data[0].get("attributes", {})
    items = attributes.get("productionPerGroupMbaHour", [])

    if not items:
        return pd.DataFrame(columns=["price_area", "production_group", "start_time", "quantity_kwh"])

    df = (
        pd.json_normalize(items)[["priceArea", "productionGroup", "startTime", "quantityKwh"]]
        .rename(columns={
            "priceArea": "price_area",
            "productionGroup": "production_group",
            "startTime": "start_time",
            "quantityKwh": "quantity_kwh",
        })
    )

    df["start_time"] = pd.to_datetime(df["start_time"], utc=True, errors="coerce")
    return df






all_chunks = []

for area in PRICE_AREAS:
    for ym in MONTHS:
        for g in PROD_GROUPS:
            try:
                dfm = fetch_month_one_group(area, ym, g)
                if not dfm.empty:
                    all_chunks.append(dfm)
                else:
                    print(f"Empty result for {area} {ym} {g}")
            except Exception as e:
                print(f"Failed for {area} {ym} {g}: {e}")

if all_chunks:
    raw_df = pd.concat(all_chunks, ignore_index=True)
else:
    raw_df = pd.DataFrame(columns=["price_area", "production_group", "start_time", "quantity_kwh"])

print("Total number of rows and columns:", raw_df.shape)
raw_df.head()






out_path = Path("/home/lse/Documents/IND320/liserochat-ind320-dashboard/app/data/elhub_production_2021_raw.csv")
out_path.parent.mkdir(parents=True, exist_ok=True)

if raw_df.empty:
    print("No CSV file was created.")
else:
    raw_df.to_csv(out_path, index=False)
    print(f"CSV file successfully saved to: {out_path.resolve()}")





# Stop any previous Spark session if running
try:
    spark.stop()
except Exception:
    pass

# Create a new Spark session configured for Cassandra
spark = (
    SparkSession.builder
    .appName("IND320-Elhub-2021")
    .config("spark.jars.packages", "com.datastax.spark:spark-cassandra-connector_2.13:3.5.0")
    .config("spark.cassandra.connection.host", "127.0.0.1")
    .config("spark.cassandra.connection.port", "9042")
    .config("spark.cassandra.output.consistency.level", "LOCAL_ONE")
    .getOrCreate()
)

spark





# Check that the dataframe is ready for Cassandra
if "raw_df" not in globals():
    raise ValueError("raw_df is not defined. Please run the previous steps first.")

if raw_df.empty:
    raise ValueError("raw_df is empty. Fetch data from the API before continuing.")

required_cols = ["price_area", "production_group", "start_time", "quantity_kwh"]
for col in required_cols:
    if col not in raw_df.columns:
        raise ValueError(f"Missing column: {col}")

raw_df.head()






# Set Cassandra connection parameters for Spark
spark.conf.set("spark.cassandra.connection.host", "127.0.0.1")
spark.conf.set("spark.cassandra.connection.port", "9042")
spark.conf.set("spark.cassandra.output.consistency.level", "LOCAL_ONE")

# Basic checks before writing
print("Columns:", raw_df.columns.tolist())
print("Types:\n", raw_df.dtypes.head())
print("Rows:", len(raw_df))

# Convert pandas dataframe to Spark dataframe
sdf = spark.createDataFrame(raw_df)

# Write to Cassandra
try:
    (
        sdf.write
        .format("org.apache.spark.sql.cassandra")
        .mode("append")
        .options(keyspace="elhub", table="production_2021")
        .save()
    )
    print("Data successfully written to elhub.production_2021")
except Exception as e:
    print("Write failed:", e)
    raise






# Read data from Cassandra
sdf_check = (
    spark.read
    .format("org.apache.spark.sql.cassandra")
    .options(keyspace="elhub", table="production_2021")
    .load()
    .select("price_area", "production_group", "start_time", "quantity_kwh")
)

# Convert to pandas dataframe
pdf = sdf_check.toPandas()

print("Data shape:", pdf.shape)
pdf.head()






# Connect to MongoDB Atlas
client = MongoClient(MONGO_URI)
db = client[MONGO_DB]
collection = db[MONGO_COLLECTION]

# Load the cleaned dataset from the CSV created earlier
csv_path = "/home/lse/Documents/IND320/liserochat-ind320-dashboard/app/data/elhub_production_2021_raw.csv"
df = pd.read_csv(csv_path)

# Convert timestamp columns if they exist
if "start_time" in df.columns:
    df["start_time"] = pd.to_datetime(df["start_time"], utc=True, errors="coerce")

# Convert DataFrame to a list of MongoDB documents
records = df.to_dict("records")

# Insert data 
if records:
    collection.insert_many(records)
    print(f"Inserted {len(records)} documents into {MONGO_DB}.{MONGO_COLLECTION}")
else:
    print("No records to insert.")

# Check
print("Example document:", collection.find_one())





docs = list(collection.find({}, {"_id": 0})) 
df_all = pd.DataFrame(docs)

print("Loaded shape:", df_all.shape)
df_all.head()





# Data cleaning and type conversion 
df_all["start_time"] = pd.to_datetime(df_all["start_time"], errors="coerce", utc=True)
df_all["quantity_kwh"] = pd.to_numeric(df_all["quantity_kwh"], errors="coerce")

# Remove rows without valid timestamp or quantity
df_all = df_all.dropna(subset=["start_time", "quantity_kwh"])

# Add derived columns for easier filtering
df_all["year"] = df_all["start_time"].dt.year
df_all["month"] = df_all["start_time"].dt.month

# Select target zone and year for analysis 
PRICE_AREA = "NO1"   
YEAR = 2021
MONTH = 1          

# Filter dataset for selected area and year
df_area = df_all[
    (df_all["price_area"] == PRICE_AREA) &
    (df_all["year"] == YEAR)
].copy()

#  Yearly aggregation per production group 
df_yearly_by_group = (
    df_area
    .groupby("production_group")["quantity_kwh"]
    .sum()
    .sort_values(ascending=False)
    .reset_index()
)

print("Yearly production per group (for pie chart):")
display(df_yearly_by_group.head())

# Time series for January (for line chart) ---
df_jan = df_area[df_area["month"] == MONTH].copy()

# Pivot: one column per production group, index = start_time
df_timeseries_jan = (
    df_jan
    .groupby(["start_time", "production_group"])["quantity_kwh"]
    .sum()
    .reset_index()
    .pivot(
        index="start_time",
        columns="production_group",
        values="quantity_kwh"
    )
    .sort_index()
)

print("January time series (for line chart):")
df_timeseries_jan.head()






# Pie chart : yearly mix, percentages in legend ---

fig_pie, ax_pie = plt.subplots(figsize=(6, 4))

# values and labels
values = df_yearly_by_group["quantity_kwh"]
labels = df_yearly_by_group["production_group"]

# percentage
total = values.sum()
percentages = (values / total * 100).round(1)
legend_labels = [f"{g} ({p:.1f}%)" for g, p in zip(labels, percentages)]

# camembert without labels or internal text
wedges, _ = ax_pie.pie(values, labels=None, startangle=90, counterclock=False)

ax_pie.set_title(f"Total production by source, {PRICE_AREA}, {YEAR}")
ax_pie.axis("equal")

# legend with percentages
ax_pie.legend(
    wedges,
    legend_labels,
    title="production_group",
    loc="center left",
    bbox_to_anchor=(1, 0.5)
)

plt.tight_layout()
plt.show()


# Line chart : hourly evolution for January ---

df_timeseries_month = df_timeseries_jan.copy()

fig_line, ax_line = plt.subplots(figsize=(10, 4))

df_timeseries_month.plot(ax=ax_line)

ax_line.set_title(f"Hourly production, {PRICE_AREA}, {YEAR}-{MONTH:02d}")
ax_line.set_ylabel("kWh")
ax_line.set_xlabel("Time (UTC)")

ax_line.legend(
    title="production_group",
    loc="upper center",
    bbox_to_anchor=(0.5, -0.2),
    ncol=3,
    fontsize="small"
)

plt.tight_layout()
plt.show()
